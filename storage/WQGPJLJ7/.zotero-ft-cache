
Publication loaded
error_outline
JavaScript disabled

You have to enable JavaScript in your browser's settings in order to use the eReader.

Or try downloading the content offline
DOWNLOAD
info list link

    navigate_before BACK info Details
    navigate_before BACK list Outline
    navigate_before BACK perm_media Materials
    navigate_before BACK link Links

open_in_new View in Publisher's site
PDF
Page 1 / 12
format_size
fullscreen remove_circle_outline add_circle_outline
search
group_add
more_horiz
get_app
BIOPHYSICS AND COMPUTATIONAL BIOLOGY COMPUTER SCIENCES Neural networks to learn protein sequence–function relationships from deep mutational scanning data Sam Gelman a,b , Sarah A. Fahlberg c , Pete Heinzelman c , Philip A. Romero c,1,2 , and Anthony Gitter a,b,d,1,2 a Department of Computer Sciences, University of Wisconsin–Madison, Madison, WI 53706; b Morgridge Institute for Research, Madison, WI 53715; c Department of Biochemistry, University of Wisconsin–Madison, Madison, WI 53706; and d Department of Biostatistics and Medical Informatics, University of Wisconsin–Madison, Madison, WI 53792 Edited by Andrej Sali, University of California, San Francisco, CA, and approved October 15, 2021 (received for review March 12, 2021) The mapping from protein sequence to function is highly complex, making it challenging to predict how sequence changes will affect a protein’s behavior and properties. We present a supervised deep learning framework to learn the sequence–function mapping from deep mutational scanning data and make predictions for new, un- characterized sequence variants. We test multiple neural network architectures, including a graph convolutional network that in- corporates protein structure, to explore how a network’s internal representation affects its ability to learn the sequence–function mapping. Our supervised learning approach displays superior per- formance over physics-based and unsupervised prediction meth- ods. We find that networks that capture nonlinear interactions and share parameters across sequence positions are important for learning the relationship between sequence and function. Further analysis of the trained models reveals the networks’ ability to learn biologically meaningful information about protein structure and mechanism. Finally, we demonstrate the models’ ability to navigate sequence space and design new proteins beyond the training set. We applied the protein G B1 domain (GB1) models to design a sequence that binds to immunoglobulin G with sub- stantially higher affinity than wild-type GB1. protein engineering | deep learning | convolutional neural network U nderstanding the mapping from protein sequence to func- tion is important for describing natural evolutionary pro- cesses, diagnosing genetic disease, and designing new proteins with useful properties. This mapping is shaped by thousands of intricate molecular interactions, dynamic conformational ensem- bles, and nonlinear relationships between biophysical properties. These highly complex features make it challenging to model and predict how changes in amino acid sequence affect function. The volume of protein data has exploded over the last decade with advances in DNA sequencing, three-dimensional structure determination, and high-throughput screening. With these in- creasing data, statistics and machine learning approaches have emerged as powerful methods to understand the complex map- ping from protein sequence to function. Unsupervised learning methods such as EVmutation (1) and DeepSequence (2) are trained on large alignments of evolutionarily related protein sequences. These methods can model a protein family’s native function, but they are not capable of predicting specific pro- tein properties that were not subject to long-term evolutionary selection. In contrast, supervised methods learn the mapping to a specific protein property directly from sequence–function examples. Many prior supervised learning approaches have limi- tations, such as the inability to capture nonlinear interactions (3, 4), poor scalability to large datasets (5), making predictions only for single-mutation variants (6), or a lack of available code (7). Other learning methods leverage multiple sequence alignments and databases of annotated genetic variants to make qualitative predictions about a mutation’s effect on organismal fitness or disease, rather than making quantitative predictions of molec- ular phenotype (8–10). There is a current need for general, easy to use supervised learning methods that can leverage large sequence–function datasets to predict specific molecular pheno- types with the high accuracy required for protein design. We address this need with a usable software framework that can be readily adopted by others for new proteins (11). We present a deep learning framework to learn protein sequence–function relationships from large-scale data generated by deep mutational scanning experiments. We train supervised neural networks to learn the mapping from sequence to function. These trained networks can then generalize to predict the functions of previously unseen sequences. We examine network architectures with different representational capabilities includ- ing linear regression, nonlinear fully connected networks, and convolutional networks that share parameters. Our supervised modeling approach displays strong predictive accuracy on five diverse deep mutational scanning datasets and compares favorably with state-of-the-art physics-based and unsupervised prediction methods. Across the different architectures tested, we find that networks that capture nonlinear interactions and share information across sequence positions display the greatest predictive performance. We explore what our neural network models have learned about proteins and how they comprehend the sequence–function mapping. The convolutional neural networks learn a protein sequence representation that Significance Understanding the relationship between protein sequence and function is necessary to design new and useful proteins with applications in bioenergy, medicine, and agriculture. The map- ping from sequence to function is tremendously complex be- cause it involves thousands of molecular interactions that are coupled over multiple lengths and timescales. We show that neural networks can learn the sequence–function mapping from large protein datasets. Neural networks are appealing for this task because they can learn complicated relationships from data, make few assumptions about the nature of the sequence–function relationship, and can learn general rules that apply across the length of the protein sequence. We demonstrate that learned models can be applied to design new proteins with properties that exceed natural sequences. Author contributions: S.G., S.A.F., P.A.R., and A.G. designed research; S.G., S.A.F., P.H., P.A.R., and A.G. performed research; S.G. and S.A.F. contributed new reagents/analytic tools; S.G., S.A.F., P.A.R., and A.G. analyzed data; and S.G., S.A.F., P.A.R., and A.G. wrote the paper. The authors declare no competing interest. This article is a PNAS Direct Submission. This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY) . 1 P.A.R. and A.G. contributed equally to this work. 2 To whom correspondence may be addressed. Email: promero2@wisc.edu or gitter@biostat.wisc.edu. This article contains supporting information online at https://www.pnas.org/lookup/ suppl/doi:10.1073/pnas.2104878118/-/DCSupplemental . Published November 23, 2021. PNAS 2021 Vol. 118 No. 48 e2104878118 https://doi.org/10.1073/pnas.2104878118 1of12
organizes sequences according to their structural and functional differences. In addition, the importance of input sequence features displays a strong correspondence to the protein’s three- dimensional structure and known key residues. Finally, we used an ensemble of the supervised learning models to design five protein G B1 domain (GB1) sequences with varying distances from the wild type. We experimentally characterized these sequences and found the top design binds to immunoglobulin G (IgG) with at least an order of magnitude higher affinity than wild-type GB1. Results A Deep Learning Framework to Model the Sequence–Function Map- ping. Neural networks are capable of learning complex, nonlin- ear input–output mappings; extracting meaningful, higher-level features from raw inputs; and generalizing from training data to new, unseen inputs (12). We develop a deep learning framework to learn from large-scale sequence–function data generated by deep mutational scanning. Deep mutational scanning data con- sist of thousands to millions of protein sequence variants that each have an associated score that quantifies their activity or fitness in a high-throughput function assay (13). We encode the protein sequences with a featurization that captures the identity and physicochemical properties of each amino acid at each posi- tion. Our approach encodes the entire protein sequence and thus can represent multimutation variants. We train a neural network to map the encoded sequences to their associated functional scores. After it is trained, the network generalizes and can predict functional scores for new, unseen protein variants (Fig. 1 A ). We test four supervised learning models to explore how different internal representations influence the ability to learn the mapping from protein sequence to function: linear regres- sion and fully connected, sequence convolutional, and graph convolutional neural networks (Fig. 1 B ). Linear regression serves as a simple baseline because it cannot capture dependencies between sites, and thus, all residues make additive contributions to the predicted fitness. Fully connected networks incorporate multiple hidden layers and nonlinear activation functions, enabling them to learn complex nonlinearities in the sequence to function mapping. In contrast to linear regression, fully connected networks are capable of modeling how combinations of residues jointly affect function beyond simple additive effects. These nonadditive effects are known as mutational epistasis (14, 15). Neither linear regression nor fully connected networks are able to learn meaningful weights for amino acid substitutions that are not directly observed in the training set. Convolutional neural networks have parameter sharing ar- chitectures that enable them to learn higher-level features that generalize across different sequence positions. They learn convo- lutional filters that identify patterns across different parts of the input. For example, a filter may learn to recognize the alternating pattern of polar and nonpolar amino acids commonly observed in β -strands. Applying this filter would enable the network to assess β -strand propensity across the entire input sequence and relate this higher-level information to the observed protein function. Importantly, the filter parameters are shared across all sequence positions, enabling convolutional networks to make meaningful predictions for mutations that were not directly observed during training. We develop a sequence-based convolutional network that integrates local sequence information by applying filters using a sliding window across the amino acid sequence. We also develop a structure-based graph convolutional network that in- tegrates three-dimensional structural information and may allow the network to learn filters that correspond to structural motifs. The graph convolutional network applies filters to neighboring nodes in a graph representation of the protein’s structure. The protein structure graph consists of a node for each residue and an A BD C Fig. 1. Overview of our supervised learning framework. ( A ) We use sequence–function data to train a neural network that can predict the functional score of protein variants. The sequence-based input captures physicochemical and biochemical properties of amino acids and supports multiple mutations per variant. The trained network can predict functional scores for previously uncharacterized variants. ( B ) We tested linear regression and three types of neural network architectures: fully connected, sequence convolutional, and graph convolutional. ( C ) Scatterplots showing performance of trained networks on the Pab1 dataset. ( D ) Process of generating the protein structure graph for Pab1. We create the protein structure graph by computing a residue distance matrix from the protein’s three-dimensional structure, thresholding the distances, and converting the resulting contact map to an undirected graph. The s tructure graph is the core part of the graph convolutional neural network. 2of12 PNAS https://doi.org/10.1073/pnas.2104878118 Gelman et al. Neural networks to learn protein sequence–function relationships from deep mutational scanning data
help
Share access [j]
